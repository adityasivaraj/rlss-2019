{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"project_report.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8RY6I2XRDsJX","colab_type":"text"},"source":["## Reinforcement Learning: Multi-Armed Bandits\n","\n","* ECE-GY 6143 Machine Learning - Spring 2020\n","* Aditya Mohana Sivaraj (ams1803)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://drive.google.com/file/d/1m0F1QY236B1Tq9fDTOu91xMUaV9tw3I2/view?usp=sharing)\n","\n","\n","\n","The project is to implement the Multi-Armed Bandits problem, which is in the simplest of words, to find out a way to maximize one's rewards when playing the *slot machine* (A gambling system/machine in casinos), using multiple algorithms.\n"]},{"cell_type":"markdown","metadata":{"id":"fcqqNWWZDzOc","colab_type":"text"},"source":["### Overview of the Topic:\n","\n","\n","\n","The project is a replication of the Notebook created by Yannis Flet-Berliac, a Deep Reinforcement-Learning doctoral student in University of Lille, France. The notebook aims to execute the basic algorithms involved in solving the popular problem in probability theory from the 1950s based on the *slot machines in casinos* Multi-Armed Bandits problem.  The aim of the problem, basically, is to maximize the rewards from a series of slot machines, by determining which lever to pull at a particular point. Since the problem involves trade-off between exploration and exploitation, it fits aptly in the realm of machine-learning. There are a couple of algorithms explored in the notebook, namely, ϵ-greedy method, and the Upper-Confidence Bound algorithm, and the multiple variations of it. Additionally the notebook also aims to explain in a little more deeper sense by exploring the influence of different variables in the formulae."]},{"cell_type":"markdown","metadata":{"id":"av5UGpyDIW3F","colab_type":"text"},"source":["### Detailed Discussion:\n","\n","![k-armed bandit](https://miro.medium.com/max/1400/1*5q0Mihf29fftuXpKWWX2uA.png)([4](#scrollTo=YTZ2N4NMDWfv&line=18&uniqifier=1))\n","\n","\n","The notebook aims to exhibit the concept of Multi-Armed Bandit worked using Reinforcement-Learning. The problem was given the spotlight in the 1950s by mathematician Herbert Robbins as a way to design sequential experiments. The problem or concept originated from a type of gambling system/machine called *slot machines* that were/are present in casinos, where the participant has to pay a fee to play one round on the machine, and after the game had started, the participant has to choose one lever to pull from a set of levers, after which based on an unknown probability distribution or a blackbox algorithm the participant will be rewarded on a variable scale ranging from undesirable reward to exceedingly desirable reward. The problem here aims, through reinforcement-learning to maximize one's (participant's) reward by going through multiple trials. \n","\n","\n","Though it is not plausible to apply this on actual slot machines in casinos, since it would be considered rigging, which would be illegal, it has various useful real-life applications. It is applied in many decision making processes, that has too many variables for humans to process, like in research organizations whether private research and development consultants or pharmaceutical companies that has to decide how to allocate its resources and on which projects, in career consultancies that has to include the various talents and qualifications into account in its decision making, financial decision making on a larger scale which has to take in an alarming amount of variables into consideration, etc.\n","\n","\n","![ϵ-greedy](https://miro.medium.com/max/1400/1*_5dltx4BcI8rRmCK2Sq_kw.png)([9](scrollTo=YTZ2N4NMDWfv&line=18&uniqifier=1))\n","\n","One of the algorithms explored in this notebook is the ϵ-greedy (epsilon-greedy)method. To explain this algorithm we first need to understand the notion of trade-off between exploration vs exploitation. Here exploration refers to the agent (decision making system) its apparent knowledge of each action, hoping to improve its long-term benefits. And, exploitation refers to the agent deciding to maximize its reward at each point, using its current estimates. The agent cannot seek to have both run simultaneously, and also running only one policy would lead to huge undesirable results. So it has to decide when to choose either of those policies, in other words *trade-off*. The epsilon value is the value of the probability of choosing which option to choose in case of exploration.\n","\n","\n","And, the other algorithm explored in this notebook is Upper Bound Confidence (UCB) algorithm. Since the algorithm can be extended based on the parameter $\\alpha$ defined differently. We will call the original one UCB1. The algorithm is based on the following equation:\n"," \n","\n","$$A_{t} = \\underset{a}{\\text{argmax}} \\left[\\hat{\\mu}_a(t-1) + \\sqrt{\\frac{\\alpha \\log(t)}{N_a(t-1)}}\\right],$$\n","where $\\hat{\\mu}_a(t)$ is the empirical mean of arm $a$ after $t$ rounds and $N_a(t)$ is the number of selections of arm $a$ till that time. \n","\n","UCB1 was originally proposed with $\\alpha = 2$. Its analysis was later refined to allow for $\\alpha > 1/2$ (see [here](http://sbubeck.com/Bubeckthesis.pdf) or [here](https://hal.archives-ouvertes.fr/hal-00738209/file/klucb.pdf)).\n","\n","The next variation is done by having the rewards bounded and hence the tweaks follows naturally. And thus we get the following modification of UCB(α) based on parameter $\\alpha$:\n","\n","$$A_{t+1} = \\text{argmin}_{a} \\left\\{N_a(t) : a \\in \\text{argmax}_{a} \\min\\left[\\mathrm{UCB}_a(t) , 1\\right] \\right\\}$$\n","\n","The next and final variant can be also be considered an alternative. We consider having high probability guarantees. And, hence, we consider the parameter $\\delta$ to be small like 0.05:\n","\n","$$\\widetilde{\\mathrm{UCB}}_a(t) = \\hat{\\mu}_a(t) + \\sqrt{\\frac{1+\\frac{1}{N_a(t)}}{2N_a(t)}\\ln\\left(\\frac{K\\sqrt{N_a(t) + 1}}{\\delta}\\right)}$$\n","\n","To explain the aforementioned algorithms further we explore the influence of the various parameters. In the $\\epsilon$-greedy method we do this by following the policy of exploration and exploitation separately (purely).\n","\n","In the case of UCB we consider a case with an underlying Bernoulli Probability Distribution, it is determined that regret is asymptotically no smaller than\n","\n","$$\\left(\\sum_{a ,\\Delta_a > 0} \\frac{\\Delta_a}{\\mathrm{kl}(\\mu_a,\\mu_\\star)}\\right) \\log(T),$$\n","where $\\mathrm{kl}(\\mu_a,\\mu_\\star)$ is the Kullback-Leibler divergence between the Bernoulli distribution of parameter $\\mu_a$ and the Bernoulli distribution of parameter $\\mu_\\star$. The constant in front of the $\\log(T)$ may be called the **complexity** of the bandit problem. \n","\n","Thus the kl-UCB is the lower bound. And hence the upper bound is:\n","$$\\mathrm{UCB}_a(t) = \\max \\left\\{ q : N_a(t) \\mathrm{kl}\\left(\\hat{\\mu}_a(t),q\\right) \\leq \\log(t) + c\\log\\log(t) \\right\\},$$\n","where $\\mathrm{kl}(\\mu,\\mu')$ is the KL-divergence between the distribution with mean $\\mu$ and that of mean $\\mu'$ in some exponential family and $c$ is some real parameter (often chosen to be zero in practice).\n","\n","The Bernoulli KL-divergence:  $\\mathrm{kl}(x,y) = x\\log(x/y) + (1-x)\\log((1-x)/(1-y)).$\n","\n","**Additionally, we also explore a Bayesian Strategy: Thompson Sampling.**\n","\n","In the Bernoulli case, it is proved to be asymptotically optimal. Letting $\\pi_a(t)$ denote the posterior distribution on arm $a$ after $t$ rounds, the algorithm can be implemented as follows: \n","\n","$$\\forall a, \\theta_a(t) \\sim \\pi_a(t-1), \\ \\ \\ A_{t} = \\underset{a}{\\text{argmax }} \\theta_a(t)$$\n","\n","$$\\implies \\pi_a(t) = \\mathrm{Beta}\\left(\\alpha + S_a(t), \\beta + N_a(t) - S_a(t)\\right)$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xhkaEhkTJy7P","colab_type":"text"},"source":["### Replication:\n","\n","In this part we aim to replicate and test the notebook. \n","\n","Since, originally the notebook is present in a completely different directory than the self-defined libraries, they had to be included in the temporary directory that this notebook is present in.\n","\n","As a good practice this notebook aims to explore the influence of the different parameters involved in the algorithms, and to test the robustness of these algorithms.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YTZ2N4NMDWfv","colab_type":"text"},"source":["References and Bibliography:\n","\n","1. Github - rlss-2019, by Yannis Flet-Derliac. ([Link](https://github.com/yfletberliac/rlss-2019))\n","\n","2. Reinforcement Learning: An Introduction - 2nd edition, by Richard S. Sutton and Andrew G. Barto. ([Link](https://github.com/adityasivaraj/rlss-2019/raw/master/Richard%20S.%20Sutton_%20Andrew%20G%20Barto%20-%20Reinforcement%20Learning_%20An%20Introduction-Bradford%20Book%20(2018).pdf))\n","\n","3. Multi-Armed Bandits and Reinforcement Learning: A Gentle Introduction to the Classic Problem with Python Examples, by Christian Hubbs - Towards Data Science. ([Link](https://towardsdatascience.com/multi-armed-bandits-and-reinforcement-learning-dc9001dcb8da))\n","\n","4. Reinforcement Learning — Multi-Arm Bandit Implementation\n","Comparison between ϵ-greedy and UCB, by Jeremy Zhang - Towards Data Science. ([Link](https://towardsdatascience.com/reinforcement-learning-multi-arm-bandit-implementation-5399ef67b24b))\n","\n","5. Multi-Armed bandit. ([Link](https://en.wikipedia.org/wiki/Multi-armed_bandit))\n","\n","6. Kullback-Leibler divergence. ([Link](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence))\n","\n","7. Bandit Process and Dynamic Allocation Indices. ([Link](https://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Gittins:1979.pdf))\n","\n","8. Epsilon-Greedy Algorithm in Reinforcement Learning, by user samishawl. ([Link](https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/))\n","\n","9. The Epsilon-Greedy Algorithm for Reinforcement Learning, by Avery Parkinson. ([Link](https://medium.com/analytics-vidhya/the-epsilon-greedy-algorithm-for-reinforcement-learning-5fe6f96dc870))"]}]}